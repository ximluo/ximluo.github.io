"use strict";(self.webpackChunkportfolio=self.webpackChunkportfolio||[]).push([[125],{3268:(e,t,a)=>{a.d(t,{Ay:()=>n});const i=[...[{id:"petsteps",name:"PetSteps",image:"/images/petsteps-app.png",description:"PetSteps is a mobile app that keeps users active by caring for a virtual pet. Real world walking boosts your pet\u2019s mood and health. It won the Adobe Digital Edge Standout Prize and was featured at Adobe MAX 2024 for its use of AI and AR in fitness. Adobe also featured PetSteps in a marketing campaign.",languages:["Swift","SwiftUI"],categories:["software","mobile","AI","AR"],sections:[{text:"PetSteps won the Digital Edge Standout Prize at Adobe MAX 2024. Adobe highlighted its originality, strong execution, and smart use of AI and AR to support real health goals."},{image:"/images/adobe-max-2024.jpeg"},{text:"Users walk to earn coins, unlock pets and items, and interact with pets in augmented reality. The app tracks distance, pace, and duration with CoreMotion and HealthKit, and shows progress with simple charts. Coach Fluff gives personalized tips using the Gemini API. Weather updates use Open Meteo and Nominatim with CoreLocation."},{text:"Built in Swift and SwiftUI with MVVM. AR uses RealityKit. Coaching uses Gemini with goal tracking and custom prompts. Data comes from CoreMotion, HealthKit, and CoreLocation. Storage uses CoreData. Async data handling keeps the UI smooth with live sensors and API calls."},{text:"The core loop is simple: move more and your pet is happy and energized. Slow down and your pet gets tired and hungry. That emotional feedback helps build habits. Bringing your pet into the real world also adds another layer of emotional bonding."},{text:"PetSteps was built for people who struggle with motivation. It makes exercise feel playful, personal, and easy to stick with."}]},{id:"mini-minecraft",name:"Mini Minecraft",image:"/images/mini-minecraft.gif",description:"Custom built 3D voxel game engine inspired by Minecraft, featuring procedural terrain, day night cycles, dynamic lighting, instanced rendering, and a post processing pipeline. I implemented the player physics, raycast based block interaction, and immersive visual effects such as a time interpolated sky with sun arcs, distance fog blending, and a post process system for water and lava overlays.",languages:["C++","OpenGL","GLSL"],categories:["graphics"],sections:[{video:"https://www.youtube.com/embed/kr7ze2p7Tx8?si=f6Pqu8gN6oUHypdi"},{text:"A 3D voxel game engine inspired by Minecraft, built with C++ and OpenGL. It features procedural terrain generation, efficient chunk streaming, day night lighting, and a custom shader pipeline."},{text:"Physics system supports flying and grounded movement, with gravity, acceleration, and collision detection. Collisions are handled per axis for smooth sliding. Precise block interaction using ray casting from the camera, enabling players to mine and place blocks with pixel accuracy."},{text:"For the day night cycle, procedurally animated the sun\u2019s arc and sky color using GLSL, interpolating between time intervals to simulate realistic transitions. Lighting was synced with the sun\u2019s direction and color. Distance based fog adjusts dynamically to sky hues using smoothstep blending, creating a seamless atmospheric fade at horizon level."},{text:"Cave generation system using 3D Perlin noise, populating underground layers with air pockets, lava pools, and bedrock. Optimized sampling by bounding height ranges during development. Post processing pipeline to tint the screen when entering water or lava, using framebuffer textures and fullscreen shaders."}]},{id:"penn-mobile",name:"Penn Mobile",image:"/images/mobile.png",description:"Penn Mobile is the University of Pennsylvania\u2019s official student life app, serving 20,000+ users. Developed by Penn Labs, it brings campus essentials like dining hours, GSR reservations, laundry availability, and student resources directly to students\u2019 phones.",languages:["Swift","SwiftUI"],categories:["software","mobile","iOS","student tools"],sections:[{text:"Learn more at [pennlabs.org/products/penn-mobile](https://pennlabs.org/products/penn-mobile)."},{text:"With Penn Mobile, students can reserve study rooms across campus, view dining hall menus and hours, check real time laundry availability, and access essential university resources, all from one centralized app."},{text:"Features include push notifications for upcoming reservations, daily laundry activity graphs, campus wide contact info, access to the Daily Pennsylvanian feed, and more. It\u2019s designed to make campus life easier, faster, and more mobile friendly."}]},{id:"penn-capsule",name:"Capsule",image:"/images/capsule-open.gif",description:"Penn Capsule transforms photo collections into interactive 3D time capsules that unlock on a specified date. Users customize capsule appearance, upload memories, and later experience a dynamic timelapse of their photos in a 3D environment.",languages:["TypeScript","Node.js","AWS S3","MongoDB","React Three Fiber"],categories:["software","web","3D","fullstack"],sections:[{text:"On the frontend, React Three Fiber renders the 3D capsules. Capsules animate open, revealing embedded photos arranged chronologically around the interior. Users interact via orbit controls in an immersive 3D view."},{text:"On the backend, handle file uploads server side to AWS S3 via the AWS SDK, define MongoDB collections for Users and Time Capsules using the native driver, secure routes with JWT, and enable Google OAuth single sign on."},{image:"/images/capsule-upload.gif"},{text:"Upload memories into a capsule by drag and drop or file picker. Each upload is handled on the server and streamed directly to S3 before metadata is recorded."},{text:"Toggle view switches between traditional list layout and 3D capsule gallery. Real time previews animate in WebGL using Spline imported scenes."},{text:"Create a new capsule by specifying a name, unlock date, and visual theme. The API returns capsule data which the UI immediately renders via React state."},{image:"/images/capsule-customize.gif"},{text:"Customize appearance with a decorator service. Color filters are applied to capsule textures."},{image:"/images/capsule-open.gif"},{text:"Open a capsule to see all memories."},{video:"/images/PennCapsule.pdf"}]},{id:"pbr-renderer",name:"Real time Physically Based Renderer",image:"/images/pbr.png",description:"A real time shading project from Penn\u2019s Advanced Rendering course. Implements the energy conserving Cook Torrance microfacet BRDF using the Trowbridge Reitz GGX distribution, Schlick\u2019s Fresnel approximation, and the Smith Schlick GGX geometry term.",languages:["C++","GLSL","OpenGL"],categories:["graphics","rendering"],sections:[{text:"The PBR shader screenshot shows the microfacet model in action. Metallicness is toggled between zero and one via GUI sliders, and roughness is held around 0 to illustrate reflectiveness."},{image:"/images/pbr-gui.png"},{text:"All lighting and material calculations run in the fragment shader. Material properties like albedo, metallicness, and roughness are sampled from textures when available or controlled with GUI sliders otherwise."},{text:"Image based lighting uses two precomputed cubemaps: one for diffuse irradiance and one for glossy irradiance. The shader samples the diffuse map using surface normals and the glossy map using the reflected view vector."},{image:"/images/pbr-displacement.png"},{text:"Vertex displacement is driven by a height map in the vertex shader. Vertices are offset along interpolated normals before tangent space normal mapping adds fine surface detail."},{image:"/images/pbr-albedo.png"},{text:"When a model provides its own albedo texture, the shader samples its base color and feeds it into both the Lambertian diffuse irradiance and Cook Torrance specular equations for accurate material appearance."},{text:"The renderer maintains full refresh rates without dropped frames, demonstrating that even with PBR and displacement, real time performance is achievable."},{text:"For full implementation details, see [Real Shading in Unreal Engine 4](https://cdn2.unrealengine.com/Resources/files/2013SiggraphPresentationsNotes-26915738.pdf)."},{text:"Beyond the core PBR pipeline, two advanced rendering features were developed as course extensions."},{text:"Deferred Rendering and Screen Space Reflection: A multi pass deferred pipeline first renders scene attributes into a G buffer with attachments for albedo, world space normals, specular, and depth. The SSR pass reconstructs view space positions and normals, reflects view vectors, and performs iterative ray marching in screen space to sample G buffer data. A binary search refines intersection points before separable Gaussian blur applies glossy falloff. The final composite blends these reflections with direct Cook Torrance lighting."},{image:"/images/pbr-ssr.png"},{text:"Signed Distance Fields and Subsurface Scattering: A ray marched SDF shader steps along view rays using the sceneSDF composed of primitive SDF operations. Upon hit detection, the Cook Torrance BSDF calculates local lighting. Subsurface scattering uses a thinness metric computed via opposite direction SDF queries to attenuate light transmitted through the material, blended with diffuse irradiance for translucent effects. Scene repetition applies SDF repetition functions to clone the building elements across the environment."},{image:"/images/sdf-sub.png"}]},{id:"gpu-path-tracer",name:"Monte Carlo Path Tracer",image:"/images/monte-carlo.png",description:"An offline Monte Carlo path tracer implemented in C++. Supports multiple integrators, area and point lights, BxDF models, depth of field, and a custom ray intersection engine.",languages:["C++","OpenGL","GLSL"],categories:["graphics","rendering"],sections:[{text:"This path tracer solves the light transport equation on the GPU using Monte Carlo integration, with a flexible framework for different sampling strategies and material models."},{text:"Global Illumination Integrator: The full integrator combines the MIS direct integrator with indirect bounces at each path vertex. By adding environment and interreflection contributions, it converges to a physically accurate result faster than the naive approach."},{image:"/images/pathtracer-cornell-area.png"},{text:"Cornell Box with Area Light: Uniformly sampling the ceiling light produces soft, realistic shadows and subtle red green light bleeding on the walls and floor."},{image:"/images/pathtracer-cornell-env.png"},{text:"Cornell Box with Environment Lighting: Without explicit lights, the environment map alone illuminates the scene. Multiple importance sampling ensures smooth, low variance results."},{image:"/images/pathtracer-cornell-spotlight.png"},{text:"Cornell Box with Spotlight: A cone shaped light source uses importance sampling within its solid angle for focused illumination and natural falloff."},{image:"/images/monte-carlo.png"},{text:"Earth Scene: An Earth and a moon lit by a distant environment map, rendered with the full integrator. Soft shadows, realistic color bleeding, and ambient illumination demonstrate the combined direct and indirect lighting."},{image:"/images/pathtracer-cornell-glass.png"},{text:"Cornell Box with Glass Ball: This scene adds a refractive glass sphere. Fresnel reflectance and transmission are computed per Schlick\u2019s approximation, resulting in realistic caustics and subtle light bending."},{image:"/images/pathtracer-global.png"},{text:"The tracer supports Lambertian diffuse, microfacet specular with GGX distribution, and refractive BxDFs. Glass like materials simulate both reflection and refraction via Fresnel equations."},{text:"Depth of field implemented with a thin lens camera model. Rays originate from a disk aperture and converge on the focal plane, producing natural focus blur. Built atop a custom ray intersection framework with Camera, Shape, and Primitive classes. Russian roulette termination and configurable bounce limits prevent runaway computation. Below are some more integrator examples:"},{text:"Naive Integrator: The simplest integrator casts random hemisphere samples at each bounce and returns lighting only when a ray directly hits a light source. Renders converge very slowly and show noticeable grain at low sample counts."},{image:"/images/pathtracer-naive.png"},{text:"Direct Lighting Integrator: This version samples scene lights explicitly, choosing random points on area or point lights to compute direct contributions. Shadows and highlights are crisp, but indirect bounces are not accounted for."},{image:"/images/pathtracer-direct.png"},{text:"Multiple Importance Sampling Integrator: To reduce variance across lights and materials, this integrator blends BSDF sampling with light sampling via the power heuristic. It produces cleaner direct lighting and fewer noise artifacts on glossy surfaces. Compare the results of MIS (left) with Naive Integrator only (center) and with Direct Integrator only (right)."},{image:"/images/pathtracer-mis.png"}]}],...[{id:"rewind",name:"Rewind",image:"/images/rewind.png",description:"Rewind is an AR memory recall app that won the InterSystems Challenge at HackMIT 2024, integrated with the Apple Vision Pro. It uses Gaussian splatting for 3D scene generation and a LangChain based RAG pipeline with InterSystems IRIS Vector Search for natural language memory queries.",languages:["React","WebXR","Python","LangChain","IRIS Vector Search"],categories:["software","AR","AI","healthtech"],sections:[{text:"Gaussian Splatting 3D Scene Generation: We convert user video clips into point cloud environments using a custom Python pipeline and render them in real time with Three.js and WebXR. Users can navigate their memories in 360 degree AR."},{text:"Backend Architecture: The Flask API manages user sessions, stores memory metadata in a relational SQL database, and coordinates scene jobs. Each memory record includes timestamps, tags, and AR scene links."},{text:"RAG with Vector Search: We embed memory metadata and textual notes into vectors using LangChain embedding models. These vectors are indexed in InterSystems IRIS Vector Search. At query time, user questions are embedded, top K matching memories are retrieved, and the app surfaces exact moments, for example, \u201cShow my graduation day\u201d."}]},{id:"web-experiments",name:"Web Exploratorium",image:"/images/web-tv.gif",description:"Web Exploratorium is a playground of mini interactive experiments built with React Three Fiber and Vite. Explore a 3D TV model with clickable hotspots that launch creative demos powered by p5.js and smooth CSS animations.",languages:["JavaScript","Vite","React Three Fiber"],categories:["software","web","interactive","3D"],sections:[{image:"/images/web-zoom.gif"},{text:"The main interface is a 3D TV rendered with React Three Fiber. Hovering circles indicate demos. Click to zoom into each screen. Screen content textures drive camera positioning calculations for smooth transitions."},{text:"Dive into the demos on the live site: [Web Exploratorium Live](https://artofthewebxl.github.io)"},{image:"/images/web-maze.gif"},{text:"Maze Generation leverages recursive backtracking in p5.js to carve perfect mazes on a grid. Real time animation shows the algorithm exploring and backtracking."},{image:"/images/web-tree.gif"},{text:"Procedural Tree Generation uses p5.js and recursion to draw branching fractals. Adjustable parameters let users explore infinitely varied tree structures."},{image:"/images/web-gallery.gif"},{text:"The CSS Image Gallery features smooth transitions and hover effects. Thumbnails expand and slide fluidly using CSS variables and keyframe animations."},{image:"/images/web-clock.gif"},{text:"Pixel Clock is a canvas based timepiece built in p5.js."},{image:"/images/web-ascii.gif"},{text:"Using the characters in the Chinese phrase, \u51b0\u5c71\u4e00\u89d2 (b\u012bng sh\u0101n y\u012b ji\u01ceo), which means \u201cThe tip of an iceberg\u201d, to create an image of an iceberg. Part of the image should be hidden, which reflects the meaning of the phrase."},{image:"/images/web-still.gif"},{text:"The CSS still life showcases an animated still life composition created using CSS."}]},{id:"lost-at-penn",name:"Lost@Penn",image:"/images/lost.png",description:"Lost@Penn is a web platform that helps the Penn community report and find lost items across campus. Built with React.js, Firebase, and Tailwind, it offers a centralized, easy to use solution to a common student problem. Through user research and iterative design, we created a trustworthy and intuitive platform focused on usability and real needs.",languages:["React.js","Tailwind","Firebase"],categories:["software","web","ux","community"],sections:[{video:"https://www.youtube.com/embed/DxMMO6Qa638?si=4tPosHJ_6WKeH1kH"},{text:"Lost@Penn was inspired by the shared frustration of losing items on campus without knowing where to turn. There was no centralized system to report or search for missing belongings, so we set out to build one. We started by interviewing Penn students to understand the pain points and features they needed most."},{text:"Lost@Penn was built with React.js for its component based architecture, Firebase for backend services and real time syncing, Tailwind and Bootstrap for responsive design, and Figma for prototyping and iteration. We focused on building a fast, usable interface with strong visual hierarchy and minimal friction. From the user interviews to the final prototype, every step of the process was grounded in solving a real problem for a real community."},{text:"Read more at: [Lost@Penn Case Study](https://medium.com/@ximingluo/lost-penn-863d1706a193)"},{video:"/images/lost.pdf"}]},{id:"neuroscent",name:"NeuroScent",image:"/images/neuroscent.jpg",description:"NeuroScent is an XR biofeedback system that promotes mental wellbeing, combining olfaction, vision, and biosensing. Built for MIT Reality Hack 2025, it won the Hardware: Smart Sensing prize and Best Use of OpenBCI.",languages:["C#","Unity","Arduino","Fusion 360"],categories:["graphics","XR","hardware","biosensing"],sections:[{text:"NeuroScent integrates scent delivery and physiological sensing to extend XR immersion beyond vision and sound. We used the Varjo headset for visual feedback and OpenBCI Galea for EEG, PPG, and EMG biosignals, creating a holistic biofeedback loop."},{text:"The system captures EEG, EMG, and heart rate data from Galea, analyzes real time mental state, and triggers adaptive scent releases and visual effects to guide users toward calm or focus."},{text:"Harvested ultrasonic atomizers from consumer diffusers, controlled them with relays on an ESP32, and designed a Fusion 360 airflow chamber. Inspired by [Nebula: An Affordable Open Source and Autonomous Olfactory Display for VR Headsets](https://hal.science/hal-03838757v1/file/Nebula_VRST_2022%20%281%29.pdf), built a compact olfactory module."},{text:"In Unity, C# scripts stream Galea biosignals over USB serial. Shader Graph, particle systems, and animation scripts map biofeedback thresholds to scent and visual triggers in real time."},{video:"https://player.vimeo.com/video/1059625069"},{text:"Optimized performance by reducing draw calls, enabling occlusion culling, and using Unity URP with baked lighting. Simplified geometry and LOD management maximizing FPS on Varjo Aero."},{text:"Next steps include expanding immersive sequences, adding more scent channel permutations, and exploring clinical applications like patient relaxation during anesthesia with targeted biofeedback driven scent delivery."}]},{id:"vr-meta-quest-experiences",name:"Portals & DreamScape VR",image:"/images/vr.png",description:"Two immersive 360 degree VR experiences for Meta Quest 3 built in Unreal Engine 5.5. Both feature spatial audio, interactive elements, and NavMesh based locomotion: \u201cPortals through the Season\u201d with seamless level streaming portals; \u201cDreamScape\u201d with grabbable crystals in a mushroom forest.",languages:["Unreal Engine","Blueprints","C++"],categories:["graphics","VR","Unreal","Quest"],sections:[{text:"Portals through the Season presents a pair of themed levels connected by interactive portals. Entering a portal uses level streaming to load and unload environments instantly. UI widgets built in Blueprints animate activation rings and transition effects. NavMesh enables smooth player movement and teleportation."},{video:"https://www.youtube.com/embed/putNy1s3eL8?si=04OUckwGQPan_b4M"},{video:"/images/Portals.pdf"},{text:"DreamScape transports users to a 360 degree mushroom forest. Players can grab and place a crystalline artifact to trigger environmental changes. Spatialized ambient sounds and sound effects respond to proximity, and NavMesh driven locomotion ensures comfort during exploration."},{video:"https://drive.google.com/file/d/1vo85unb33cchrQF0wL8lFlqO5VpXHhL6/preview"},{video:"/images/DreamScape.pdf"}]},{id:"glsl-shaders",name:"GLSL Shaders",image:"/images/glsl-mosaic.png",description:"A collection of real time GLSL shaders written in C++ and OpenGL. Each shader demonstrates a different visual effect or lighting model, including vertex displacement, lighting, and post processing filters.",languages:["C++","OpenGL","GLSL"],categories:["graphics","shaders"],sections:[{image:"/images/glsl-inflate.gif"},{text:"Vertex Inflation: A custom vertex shader that offsets each vertex along its normal, creating an expanding puffed effect. The degree of inflation is adjustable in real time."},{image:"/images/glsl-mosaic.png"},{text:"Mosaic Filter: A fragment shader that divides the screen into a grid of tiles. Each tile samples the center pixel color and applies it uniformly across the tile area, creating a pixelated mosaic effect. The tile size is adjustable in real time."},{image:"/images/glsl-blinn.png"},{text:"Blinn Phong: Per fragment lighting using ambient, diffuse, and specular components. The half vector is used for more efficient specular reflection."},{image:"/images/glsl-blur.png"},{text:"Gaussian Blur: A separable 2 pass blur filter implemented in screen space, using a weighted kernel to smooth the scene."},{image:"/images/glsl-sobel.png"},{text:"Sobel: Edge detection filter that computes the gradient magnitude across screen space color values, highlighting high frequency detail."},{image:"/images/glsl-lambert.png"},{text:"Lambert: Basic diffuse lighting using the cosine of the angle between light and surface normal. Simple and efficient per fragment shading."},{image:"/images/glsl-chrome.png"},{text:"Matcap Chrome: Uses a normal based matcap texture lookup to create a reflective metallic appearance with no dynamic lighting."},{image:"/images/glsl-red.png"},{text:"Matcap Red Plastic: Simulates plastic like material using a matcap texture with strong specular highlights and soft shading transitions."}]}],...[{id:"statistical-learning-returns",name:"Machine Learning in Asset Pricing",image:"/images/wdrp.png",description:"This Wharton Directed Reading Program project investigates whether asset returns can be predicted using modern statistical learning methods. The research evaluates the efficiency of financial markets by applying both classical econometrics and advanced machine learning models to historical stock data.",languages:["Python","R"],categories:["software","finance","machine learning","research"],sections:[{text:"Wharton Directed Reading Program, April 24, 2025: MACHINE LEARNING IN ASSET PRICING. Exploring predictability in stock returns. Lead student: Ximing Luo (CS (DMD) and Economics \u201927). Mentor: Yiwen Lu (Finance PhD, 2nd Year)."},{text:"Introduction: Challenge. Despite the efficient market hypothesis asserting that prices reflect all available information, subtle anomalies persist. Motivation. Asset prices are dynamic predictions built on evolving data, and machine learning can uncover weak nonlinear signals without strict functional assumptions."},{text:"Background and Methodology: Reviewed efficient market hypothesis in weak, semi strong, and strong forms. Revisited the fundamental asset pricing equation and CAPM as a benchmark. Designed a hybrid workflow exploring econometric models such as linear regression, ARIMA, and factor models with modern ML such as penalized regression, tree based methods, feed forward and recurrent neural networks, and CNNs for chart image feature extraction."},{text:"Data, Analysis and Empirical Findings: Used historical prices, technical and fundamental signals, and alternative sources such as news sentiment and OHLC chart images. Extracted features via PCA, NLP topic models, and CNN embeddings. Trained and validated with pseudo out of sample cross validation. Modern ML models achieved higher out of sample R squared and improved trading metrics, revealing double descent behavior in overparameterized regimes."},{text:"Implications and Future Directions: Modern ML improves return predictability and informs adaptive long short strategies, risk forecasts, and real time integration of alternative data. Future work will deepen sentiment and image based features and develop hybrid frameworks that fuse economic theory with data driven predictions to better capture evolving market dynamics."}]},{id:"hci-research-jhu",name:"Human AI Interaction Projects at JHU",image:"/images/hci.png",description:"Conducted research in the Intuitive Computing Laboratory under Dr Chien Ming Huang. Contributed to projects on two areas of human AI interaction: end to end co creation of visual stories with generative models, and apology strategies to mitigate errors in voice assistants.",languages:["Python","React.js"],categories:["software","HCI","Generative AI","User Studies"],sections:[{text:"Contributed to two projects in the Intuitive Computing Lab at Johns Hopkins University, collaborating with PhD students Victor Nikhil Antony and Amama Mahmood. One project introduces an integrated authoring system for visual story creation using LLMs and multimodal generation. The other studies how apology tone and blame assignment affect user perceptions of voice assistants after recognition errors."},{text:"ID.8: Co Creating Visual Stories with Generative AI. Built an open source React system for a multi stage authoring workflow: collaborative script generation, automated scene parsing into storyboards, and asset creation with generative models. A human in control design keeps users in charge while AI accelerates iteration. User studies found strong usability and enjoyment and produced design guidance for prompt templates, iterative co creation, and consistent AI identity management. For details, see [ID.8: Co Creating Visual Stories with Generative AI](https://doi.org/10.1145/3672277)."},{text:"Owning Mistakes Sincerely: Strategies for Mitigating AI Errors. Ran an online study with a voice based shopping assistant that varied apology style and blame attribution after homonym recognition errors. A serious apology that accepts blame improved recovery satisfaction, perceived intelligence, and likeability, while blaming others could be worse than no apology. For details, see [Owning Mistakes Sincerely: Strategies for Mitigating AI Errors](https://doi.org/10.1145/3491102.3517565)."}]},{id:"ar-mri-point-cloud",name:"AR MRI Point Cloud Visualization",image:"/images/ar-mri.png",description:"Integrated pipeline automating conversion of patient brain MRIs into high quality 3D point cloud models for augmented reality. Enables collaborative visualization and annotation on Microsoft HoloLens head mounted displays.",languages:["Python","C#"],categories:["software","Augmented Reality","Medical Imaging","Web Development"],sections:[{text:"This project streamlines a manual workflow of medical scan segmentation, mesh cleanup, and file conversion into a single web application. Physicians upload DICOM MRI sequences, automatically generate refined 3D point cloud models, and view or annotate them in real time on multiple AR headsets."},{text:"Pipeline steps include intensity threshold segmentation on 2D slices, outlier removal that drops points beyond 2.5 standard deviations from each anatomical centroid, and streaming the processed point cloud through a REST API into a Unity based Microsoft HoloLens app for immersive visualization, annotation, and manipulation."},{text:"Generated two complete point cloud models from brain MRI datasets and rendered them in AR, highlighting challenges with complex neuroanatomy. Ongoing work improves segmentation robustness, adds color coded segment views, and expands support for multi contrast MRI."},{text:"Worked with Dr. Chamith Rajapakse in Radiology and Orthopedics to optimize machine learning based MRI segmentation workflows in Python and C++ for 3D point cloud generation, and integrated real time AR visualization and annotation on Microsoft HoloLens 2 using Unity and C# for clinical diagnostics."}]}]],n=(i.map((e=>{let{sections:t,...a}=e;return a})),i)},4022:(e,t,a)=>{a.d(t,{A:()=>n});var i=a(5043);const n=function(){let{root:e=null,rootMargin:t,threshold:a,disabled:n=!1,initialValue:s=!1}=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};const[r,o]=(0,i.useState)(null),[l,d]=(0,i.useState)(s);return(0,i.useEffect)((()=>{if(n||l||!r)return;if("undefined"===typeof IntersectionObserver)return void d(!0);const i=new IntersectionObserver((e=>{e.some((e=>e.isIntersecting))&&(d(!0),i.disconnect())}),{root:e,rootMargin:t,threshold:a});return i.observe(r),()=>i.disconnect()}),[n,l,r,e,t,a]),{ref:o,hasIntersected:l,setHasIntersected:d}}}}]);
//# sourceMappingURL=125.bc149284.chunk.js.map